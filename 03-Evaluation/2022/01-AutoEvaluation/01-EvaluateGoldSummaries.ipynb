{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035213b2",
   "metadata": {},
   "source": [
    "# Compare Gold-Standard Event Summaries\n",
    "\n",
    "__Author__: Cody Buntain (cbuntain@umd.edu)\n",
    "\n",
    "## Description\n",
    "\n",
    "For automated evaluation in CrisisFACTS, we compare participant-system summaries to three additional sources of event summaries:\n",
    "\n",
    "1. Wikipedia - A simple summary of each event, though we expect these summaries are not massively useful for situational awareness, attention support, or decision making.\n",
    "\n",
    "2. ICS 209 Archive - A dataset of real daily hazard reports, gathered from Lise St. Denis. This data comes from a pre-release version of their updated NIMS database.\n",
    "    \n",
    "3. NIST Assessor Summaries - A dataset of event summaries generated by NIST assessors, where CrisisFACTS coordinators asked NIST assessors to identify and timestamp important facts from each event.\n",
    "\n",
    "An important research question concerns how well these three summaries corroborate each other. Here, we assess this question directly by assessing both ROUGE and BERTScores across pairs of summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6553dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06bdae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa3436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce70bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.12'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "import bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2337c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae84c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"gold.summaries.json.gz\", \"rb\") as in_file:\n",
    "    summaries = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082717a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17122ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"wiki.summary\", \"ics.summary\", \"nist.summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da4bc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83e4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore(\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df09f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6aec7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wiki.summary', 'ics.summary'),\n",
       " ('wiki.summary', 'nist.summary'),\n",
       " ('ics.summary', 'wiki.summary'),\n",
       " ('ics.summary', 'nist.summary'),\n",
       " ('nist.summary', 'wiki.summary'),\n",
       " ('nist.summary', 'ics.summary')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_pairs = []\n",
    "for t1 in targets:\n",
    "    for t2 in targets:\n",
    "        if t1 == t2:\n",
    "            continue\n",
    "        target_pairs.append((t1, t2))\n",
    "        \n",
    "target_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac23d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-001\n",
      "\t wiki.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-002\n",
      "\t wiki.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-003\n",
      "\t wiki.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-004\n",
      "\t wiki.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-005\n",
      "\t wiki.summary ics.summary\n",
      "\tSkipping: wiki.summary ics.summary\n",
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n",
      "\tSkipping: ics.summary wiki.summary\n",
      "\t ics.summary nist.summary\n",
      "\tSkipping: ics.summary nist.summary\n",
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n",
      "\tSkipping: nist.summary ics.summary\n",
      "CrisisFACTS-006\n",
      "\t wiki.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-007\n",
      "\t wiki.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-008\n",
      "\t wiki.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t wiki.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ics.summary nist.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary wiki.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t nist.summary ics.summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-xlarge-mnli were not used when initializing DebertaModel: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
      "/Users/cbuntain/.local/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p2c_att = torch.matmul(key_layer, torch.tensor(pos_query_layer.transpose(-1, -2), dtype=key_layer.dtype))\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for event_pair in summaries:\n",
    "    event_id = event_pair[\"eventID\"]\n",
    "    print(event_id)\n",
    "\n",
    "\n",
    "    for t1,t2 in target_pairs:\n",
    "        print(\"\\t\", t1, t2)\n",
    "        \n",
    "        if event_id == \"CrisisFACTS-005\":\n",
    "            if \"ics.summary\" in {t1, t2}:\n",
    "                print(\"\\tSkipping:\", t1,t2)\n",
    "                continue\n",
    "            \n",
    "        metric = rouge(event_pair[t1], event_pair[t2])\n",
    "        bert_metric = bert_score.score(\n",
    "            [event_pair[t1]], \n",
    "            [event_pair[t2]], \n",
    "            model_type=\"microsoft/deberta-xlarge-mnli\"\n",
    "        )\n",
    "        \n",
    "        metric.update({\n",
    "            \"bertscore_precision\": bert_metric[0],\n",
    "            \"bertscore_recall\": bert_metric[1],\n",
    "            \"bertscore_fmeasure\": bert_metric[2],\n",
    "        })\n",
    "        \n",
    "\n",
    "        this_metric_rows = [{\n",
    "            \"metric\":k, \n",
    "            \"value\":v.item(), \n",
    "            \"event\": event_id,\n",
    "            \"target\": \"%s->%s\" % (t1,t2)\n",
    "        } for k,v in metric.items()]\n",
    "        this_row_df = pd.DataFrame(this_metric_rows)\n",
    "        rows.append(this_row_df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ea6b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>event</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge1_fmeasure</td>\n",
       "      <td>0.036104</td>\n",
       "      <td>CrisisFACTS-001</td>\n",
       "      <td>wiki.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge1_precision</td>\n",
       "      <td>0.696552</td>\n",
       "      <td>CrisisFACTS-001</td>\n",
       "      <td>wiki.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rouge1_recall</td>\n",
       "      <td>0.018532</td>\n",
       "      <td>CrisisFACTS-001</td>\n",
       "      <td>wiki.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rouge2_fmeasure</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>CrisisFACTS-001</td>\n",
       "      <td>wiki.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rouge2_precision</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>CrisisFACTS-001</td>\n",
       "      <td>wiki.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rougeLsum_precision</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>CrisisFACTS-008</td>\n",
       "      <td>nist.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rougeLsum_recall</td>\n",
       "      <td>0.575581</td>\n",
       "      <td>CrisisFACTS-008</td>\n",
       "      <td>nist.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bertscore_precision</td>\n",
       "      <td>0.498946</td>\n",
       "      <td>CrisisFACTS-008</td>\n",
       "      <td>nist.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bertscore_recall</td>\n",
       "      <td>0.524793</td>\n",
       "      <td>CrisisFACTS-008</td>\n",
       "      <td>nist.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bertscore_fmeasure</td>\n",
       "      <td>0.511543</td>\n",
       "      <td>CrisisFACTS-008</td>\n",
       "      <td>nist.summary-&gt;ics.summary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>660 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 metric     value            event                     target\n",
       "0       rouge1_fmeasure  0.036104  CrisisFACTS-001  wiki.summary->ics.summary\n",
       "1      rouge1_precision  0.696552  CrisisFACTS-001  wiki.summary->ics.summary\n",
       "2         rouge1_recall  0.018532  CrisisFACTS-001  wiki.summary->ics.summary\n",
       "3       rouge2_fmeasure  0.006794  CrisisFACTS-001  wiki.summary->ics.summary\n",
       "4      rouge2_precision  0.131944  CrisisFACTS-001  wiki.summary->ics.summary\n",
       "..                  ...       ...              ...                        ...\n",
       "10  rougeLsum_precision  0.038961  CrisisFACTS-008  nist.summary->ics.summary\n",
       "11     rougeLsum_recall  0.575581  CrisisFACTS-008  nist.summary->ics.summary\n",
       "12  bertscore_precision  0.498946  CrisisFACTS-008  nist.summary->ics.summary\n",
       "13     bertscore_recall  0.524793  CrisisFACTS-008  nist.summary->ics.summary\n",
       "14   bertscore_fmeasure  0.511543  CrisisFACTS-008  nist.summary->ics.summary\n",
       "\n",
       "[660 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df = pd.concat(rows)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf846675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad0812d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ics.summary->nist.summary\n",
      "ics.summary->wiki.summary\n",
      "nist.summary->ics.summary\n",
      "nist.summary->wiki.summary\n",
      "wiki.summary->ics.summary\n",
      "wiki.summary->nist.summary\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "for label,target_group in all_df.groupby(\"target\"):\n",
    "    print(label)\n",
    "    t1,_,t2 = label.partition(\"->\")\n",
    "    \n",
    "    for metric,metric_group in target_group.groupby(\"metric\"):\n",
    "        mean_v = metric_group[\"value\"].mean()\n",
    "        \n",
    "        all_rows.append({\n",
    "            \"left\": t1,\n",
    "            \"right\": t2,\n",
    "            \"metric\": metric,\n",
    "            \"value\": mean_v\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f7fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3cb35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ics.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>bertscore_fmeasure</td>\n",
       "      <td>0.513429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ics.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>bertscore_precision</td>\n",
       "      <td>0.518837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ics.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>bertscore_recall</td>\n",
       "      <td>0.508553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ics.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>rouge1_fmeasure</td>\n",
       "      <td>0.197641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ics.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>rouge1_precision</td>\n",
       "      <td>0.233307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>rougeL_precision</td>\n",
       "      <td>0.353828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>rougeL_recall</td>\n",
       "      <td>0.042619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>rougeLsum_fmeasure</td>\n",
       "      <td>0.141913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>rougeLsum_precision</td>\n",
       "      <td>0.645438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>rougeLsum_recall</td>\n",
       "      <td>0.082855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            left         right               metric     value\n",
       "0    ics.summary  nist.summary   bertscore_fmeasure  0.513429\n",
       "1    ics.summary  nist.summary  bertscore_precision  0.518837\n",
       "2    ics.summary  nist.summary     bertscore_recall  0.508553\n",
       "3    ics.summary  nist.summary      rouge1_fmeasure  0.197641\n",
       "4    ics.summary  nist.summary     rouge1_precision  0.233307\n",
       "..           ...           ...                  ...       ...\n",
       "85  wiki.summary  nist.summary     rougeL_precision  0.353828\n",
       "86  wiki.summary  nist.summary        rougeL_recall  0.042619\n",
       "87  wiki.summary  nist.summary   rougeLsum_fmeasure  0.141913\n",
       "88  wiki.summary  nist.summary  rougeLsum_precision  0.645438\n",
       "89  wiki.summary  nist.summary     rougeLsum_recall  0.082855\n",
       "\n",
       "[90 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df = pd.DataFrame(all_rows)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce861dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "835465f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "right & ics.summary & nist.summary & wiki.summary \\\\\n",
      "left &  &  &  \\\\\n",
      "ics.summary & nan & 0.513429 & 0.488502 \\\\\n",
      "nist.summary & 0.513429 & nan & 0.536848 \\\\\n",
      "wiki.summary & 0.488502 & 0.536848 & nan \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_df = metric_df[metric_df[\"metric\"] == \"bertscore_fmeasure\"][[\"left\", \"right\", \"value\"]].pivot(\"left\", \"right\", \"value\")\n",
    "print(matrix_df.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "712ed278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ics.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>0.513429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ics.summary</td>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>0.488502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>nist.summary</td>\n",
       "      <td>ics.summary</td>\n",
       "      <td>0.513429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>nist.summary</td>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>0.536848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>ics.summary</td>\n",
       "      <td>0.488502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>wiki.summary</td>\n",
       "      <td>nist.summary</td>\n",
       "      <td>0.536848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            left         right     value\n",
       "0    ics.summary  nist.summary  0.513429\n",
       "15   ics.summary  wiki.summary  0.488502\n",
       "30  nist.summary   ics.summary  0.513429\n",
       "45  nist.summary  wiki.summary  0.536848\n",
       "60  wiki.summary   ics.summary  0.488502\n",
       "75  wiki.summary  nist.summary  0.536848"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_df[metric_df[\"metric\"] == \"bertscore_fmeasure\"][[\"left\", \"right\", \"value\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b437021f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28655aeb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047a98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d762517b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>right</th>\n",
       "      <th>ics.summary</th>\n",
       "      <th>nist.summary</th>\n",
       "      <th>wiki.summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ics.summary</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042957</td>\n",
       "      <td>0.007751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nist.summary</th>\n",
       "      <td>0.042957</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.035602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki.summary</th>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.035602</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "right         ics.summary  nist.summary  wiki.summary\n",
       "left                                                 \n",
       "ics.summary           NaN      0.042957      0.007751\n",
       "nist.summary     0.042957           NaN      0.035602\n",
       "wiki.summary     0.007751      0.035602           NaN"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_df = metric_df[metric_df[\"metric\"] == \"rouge2_fmeasure\"][[\"left\", \"right\", \"value\"]].pivot(\"left\", \"right\", \"value\")\n",
    "plt.imshow(matrix_df)\n",
    "plt.show()\n",
    "\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226c4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "310f8d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrr}\n",
      "right & ics.summary & nist.summary & wiki.summary \\\\\n",
      "left &  &  &  \\\\\n",
      "ics.summary & nan & 0.197641 & 0.057083 \\\\\n",
      "nist.summary & 0.197641 & nan & 0.150451 \\\\\n",
      "wiki.summary & 0.057083 & 0.150451 & nan \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix_df = metric_df[metric_df[\"metric\"] == \"rouge1_fmeasure\"][[\"left\", \"right\", \"value\"]].pivot(\"left\", \"right\", \"value\")\n",
    "plt.imshow(matrix_df)\n",
    "plt.show()\n",
    "\n",
    "print(matrix_df.style.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddeb6399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>right</th>\n",
       "      <th>ics.summary</th>\n",
       "      <th>nist.summary</th>\n",
       "      <th>wiki.summary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ics.summary</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.070481</td>\n",
       "      <td>0.032876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nist.summary</th>\n",
       "      <td>0.070481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.073139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wiki.summary</th>\n",
       "      <td>0.032876</td>\n",
       "      <td>0.073139</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "right         ics.summary  nist.summary  wiki.summary\n",
       "left                                                 \n",
       "ics.summary           NaN      0.070481      0.032876\n",
       "nist.summary     0.070481           NaN      0.073139\n",
       "wiki.summary     0.032876      0.073139           NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_df = metric_df[metric_df[\"metric\"] == \"rougeL_fmeasure\"][[\"left\", \"right\", \"value\"]].pivot(\"left\", \"right\", \"value\")\n",
    "plt.imshow(matrix_df)\n",
    "plt.show()\n",
    "\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b1745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
